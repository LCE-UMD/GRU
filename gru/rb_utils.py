'''
utils for bhv regression (rb)
'''

import tensorflow as tf
import numpy as np
import scipy as sp
import pandas as pd
import pickle

'''
SCORES:
'mse': mean squared error
'p': pearson correlation
's': spearman correlation
'''
SCORES = ['mse', 'p', 's']

def _get_t_score(y_hat, y, k_time):
    '''
    score as f(time)
    '''
    s = {}
    for score in SCORES:
        s[score] = np.zeros(k_time)
    
    for ii in range(k_time):
        y_i = y[ii::k_time]
        y_hat_i = y_hat[ii::k_time]
        s['mse'][ii] = tf.metrics.mse(y_hat_i, y_i)
        s['p'][ii] = np.corrcoef(y_hat_i, y_i)[0, 1]
        s['s'][ii] = sp.stats.spearmanr(y_hat_i, y_i)[0]
        
    return s

def dnn_score(model, X, Y, C, X_len, max_length, 
              clip_time, model_type, 
              return_states = False):
    """ masked accuracy for gru
    
    Args:
        model: keras model
        X: inputs
        Y: true scores
        C: clip labels
        X_len: length of sequences
        max_length: maximum length
        clip_time: length of each clip (1 x k_clips)
        model_type: 'gru'
        return_states: was return_states set to
                       true in original model?

    Return:
        s: mean scores (across time and clips)
        s_t: temporal scores (for each clip)
        Y: true scores
        Y_HAT: predicted scores
        C: clip labels
    """
    
    # mask to ignore padding
    mask = model.layers[0].compute_mask(X)
    
    # forward pass
    if return_states:
        print('Return states feature not programmed')
    else:
        #if model_type == 'gru':
        Y_HAT = tf.squeeze(model.predict(X),2)
        #else:
        #    print('Other models not programmed')
            
    # to numpy for saving
    Y = Y.copy()
    Y_HAT = Y_HAT.numpy()
    C = C.copy()
    
    # remove padding values, converts matrix to vec
    y_hat = Y_HAT[mask==True]
    y = Y[mask==True]
    c = C[mask==True]
    
    # mean scores (across all t)
    s = {}
    s['mse'] = tf.metrics.mse(y_hat,y)
    s['p'] = np.corrcoef(y_hat, y)[0,1]
    s['s'] = sp.stats.spearmanr(y_hat,y)[0]
    
    # score as a function of t
    k_clip = len(clip_time)
    s_t = {}
    for ii in range(k_clip):
        y_i = y[c == ii]
        y_hat_i = y_hat[c == ii]
        k_time = clip_time[ii]
        s_t[ii] = _get_t_score(y_hat_i, y_i, k_time)
        
    return s, s_t, Y, Y_HAT, C 

def mse(y_true, y_pred, mask):
        return tf.keras.metrics.mse(tf.boolean_mask(y_true,mask),tf.boolean_mask(y_pred,mask))
    
def Rsquared(y_true,y_pred,mask):
    y_true = tf.boolean_mask(y_true,mask)
    y_pred = tf.boolean_mask(y_pred,mask)

    tss = tf.math.reduce_sum(tf.math.square(y_true - tf.math.reduce_mean(y_true)))
    rss = tf.math.reduce_sum(tf.math.square(y_true - y_pred))

    return ((tss - rss)/tss)


"""
the following block contains code for
* munging data from results generated by
    - bhv_lstm.py, bhv_tcn.py, bhv_ff.py
"""
def _get_results(args, bhv, num_epochs = 10,k_param = 10, perm=False):
    """ load results pkl file
    
    Args:
        k_param: k_hidden if model_type = 'lstm'
                 k_wind if model_type = 'tcn'
                 k_layers if model_type = 'ff'
                 corr_thresh if model_type = 'cpm'
                 k_components if model_type = 'bbs'
    """
    res_path = (
        args.RES_DIR + 
        '/roi_%d_net_%d' %(args.roi, args.net) + 
        '_nw_%s' %(args.subnet) +
        '_bhv_%s' %(bhv) +
        '_trainsize_%d' %(args.train_size))
    perm_res_path = (
        args.RES_DIR + '_perm'
        '/roi_%d_net_%d' %(args.roi, args.net) + 
        '_nw_%s' %(args.subnet) +
        '_bhv_%s' %(bhv) +
        '_trainsize_%d' %(args.train_size))
    if args.model_type == 'gru':
        res_path += (
            '_kfold_%d_k_hidden_%d' %(args.k_fold, k_param) +
            '_k_layers_%d_batch_size_%d' %(args.k_layers, args.batch_size) +
            '_num_epochs_%d_z_%d.pkl' %(num_epochs, args.zscore))
        perm_res_path += (
            '_kfold_%d_k_hidden_%d' %(args.k_fold, k_param) +
            '_k_layers_%d_batch_size_%d' %(args.k_layers, args.batch_size) +
            '_num_epochs_%d_z_%d.pkl' %(num_epochs, args.zscore))
    elif args.model_type == 'tcn':
        res_path += (
            '_kfold_%d_k_hidden_%d' %(args.k_fold, args.k_hidden) +
            '_k_wind_%d_batch_size_%d' %(k_param, args.batch_size) +
            '_num_epochs_%d_z_%d.pkl' %(num_epochs, args.zscore))
    elif args.model_type == 'ff':
        res_path += (
            '_kfold_%d_k_hidden_%d' %(args.k_fold, args.k_hidden) +
            '_k_layers_%d_batch_size_%d' %(k_param, args.batch_size) +
            '_num_epochs_%d_z_%d.pkl' %(num_epochs, args.zscore))
    elif args.model_type == 'cpm':
        res_path += (
            '_corrthresh_%0.1f' %(k_param) +
            '_kfold_%d_z_%d.pkl' %(args.k_fold, args.zscore))
    elif args.model_type == 'bbs':
        res_path += (
            '_kcomponents_%0.1f' %(k_param) +
            '_kfold_%d_z_%d.pkl' %(args.k_fold, args.zscore))

    with open(res_path, 'rb') as f:
        r = pickle.load(f)
    if perm == True:
        with open(perm_res_path, 'rb') as f:
            r_perm = pickle.load(f)
        return r, r_perm
    else:
        return r

def get_cv_tbest(args, clip, r, score):
    """determine tbest based on cv
    
    Args:
        clip: clip name
        r: dict of results from _get_results
        score: 'p', 's', 'mse'
        
    Return:
        t_cv_tbest
        v_cv_tbest
    """
    c = args.clip_y[clip]
    A_cv = r['train_mode']

    # mean across folds
    T_cv = np.mean(A_cv['t_train_%s'%score][c], axis=0)
    V_cv = np.mean(A_cv['t_val_%s'%score][c], axis=0)
    if score=='mse':
        tbest = np.argmin(V_cv)
    else:
        tbest = np.argmax(V_cv)
     
    return T_cv[tbest], V_cv[tbest]

def get_static(args, clip, r, score,
               mode = 'train'):
    """return mean cv accuracy for cpm and bbs
    
    Args:
        clip: clip name
        r: dict of results from _get_results
        score: 'p', 's', 'mse'
        
    Return:
        t, v
    """
    c = args.clip_y[clip]
    
    if mode == 'train':
        A_cv = r['train_mode']
        # mean across folds
        t = np.nanmean(A_cv['c_train_%s'%score][c])
        v = np.nanmean(A_cv['c_val_%s'%score][c])
    else:
        A = r['test_mode']
        t = A['c_train_%s'%score][c]
        v = A['c_test_%s'%score][c]
     
    return t, v

def get_test_tbest(args, clip, r, score):
    """ determine tbest based on cv
        * return test acc at tbest
    
    Args:
        clip: clip name
        r: dict of results from _get_results
        score: 'p', 's', 'mse'
        
    Return:
        t_test_tbest
        v_test_tbest
    """
    c = args.clip_y[clip]
    A_train = r['train_mode']
    A_test = r['test_mode']

    # mean across cv folds
    T_cv = np.mean(A_train['t_train_%s'%score][c], axis=0)
    V_cv = np.mean(A_train['t_val_%s'%score][c], axis=0)
    if score=='mse':
        tbest = np.argmin(V_cv)
    else:
        tbest = np.argmax(V_cv)

    t_test = A_test['t_train_%s'%score][c]
    v_test = A_test['t_test_%s'%score][c]
        
    return t_test[tbest], v_test[tbest]

def get_test_predictions(args, clip, r, score):
    """ determine tbest based on cv
        * return predictions at tbest
    
    Args:
        clip: clip name
        r: dict of results from _get_results
        score: 'p', 's', 'mse'
        
    Return:
        y_hat: predicted scores
        y: true scores
    """
    c = args.clip_y[clip]
    A_train = r['train_mode']
    A_test = r['test_mode']

    # mean across cv folds
    T_cv = np.mean(A_train['t_train_%s'%score][c], axis=0)
    V_cv = np.mean(A_train['t_val_%s'%score][c], axis=0)
    if score=='mse':
        tbest = np.argmin(V_cv)
    else:
        tbest = np.argmax(V_cv)

    c_y = A_test['c'][:, tbest]
    y = A_test['y'][:, tbest][c_y==c]
    y_hat = A_test['y_hat'][:, tbest][c_y==c]
        
    return y_hat, y

def get_static_predictions(args, clip, r, score):
    """ return predictions for clip
    
    Args:
        clip: clip name
        r: dict of results from _get_results
        score: 'p', 's', 'mse'
        
    Return:
        y_hat: predicted scores
        y: true scores
    """
    c = args.clip_y[clip]
    A_test = r['test_mode']

    c_y = A_test['c']
    y = A_test['y'][c_y==c]
    y_hat = A_test['y_hat'][c_y==c]
        
    return y_hat, y

def cv_agg(args, bhv, k_param, 
           score, num_epochs = 10, 
           criteria = 'max'):
    """aggregate cv values across clips
    
    Args:
        bhv:
        num_epochs:
        k_hidden:
        score: 'p', 's', 'mse'
        criteria:
            max: max across clips
            mean: mean across clips
            
    Return:
        train_acc, val_acc
    """
    if args.model_type in ['gru','ff','tcn']:
        print(k_param,num_epochs)
        r = _get_results(args, bhv, num_epochs = num_epochs, 
                         k_param = k_param)
    elif args.model_type in ['cpm', 'bbs']:
        r = _get_results(args, bhv, k_param = k_param)
        
    # compute average or max score across clips
    clip_train_acc = np.zeros(len(args.clip_names))
    clip_val_acc = np.zeros(len(args.clip_names))
    for ii, clip in enumerate(args.clip_names):
        if args.model_type in ['gru','ff','tcn']:
            t, v = get_cv_tbest(args, clip, r, score)
        elif args.model_type in ['cpm', 'bbs']:
            t, v = get_static(args, clip, r, score)
        clip_train_acc[ii] = t
        clip_val_acc[ii] = v
    
    if criteria == 'max':
        star_train_acc = np.nanmax(clip_train_acc)
        star_val_acc = np.nanmax(clip_val_acc)
    elif criteria == 'mean':
        star_train_acc = np.nanmean(clip_train_acc)
        star_val_acc = np.nanmean(clip_val_acc)
    
    return star_train_acc, star_val_acc

def get_grid_acc(args, bhv, score,
                 criteria = 'max'):
    """ show cv acc for pair of hyperparams
        * average across cv, find max across time
        
    Args:
        bhv: behavioral measure
    
    Returns:
        heat_map: acc (num_epochs x k_hidden)
    """
    if args.model_type in ['gru','ff','tcn']:
        heat_map_train = np.zeros(
            (len(args.num_epochs), len(args.k_param)))
        heat_map_val = np.zeros(
            (len(args.num_epochs), len(args.k_param)))
        for ii, num_epochs in enumerate(args.num_epochs):
            for jj, k_param in enumerate(args.k_param):
                t, v = cv_agg(args, bhv, 
                              k_param, score, 
                              num_epochs = num_epochs,
                              criteria = criteria)
                heat_map_train[ii, jj] = t
                heat_map_val[ii, jj] = v
    elif args.model_type in ['cpm', 'bbs']:
        heat_map_train = np.zeros(len(args.k_param))
        heat_map_val = np.zeros(len(args.k_param))
        for ii, k_param in enumerate(args.k_param):
            t, v = cv_agg(args, bhv, 
                          k_param, score, 
                          criteria = criteria)
            heat_map_train[ii] = t
            heat_map_val[ii] = v
            
    return heat_map_train, heat_map_val

def get_best_param(args, bhv, score, 
                   criteria = 'max'):
    """ gets best hyperparameters for each behavior
        based on cv
        * average across cv, find max across time
        
    Args:
        bhv: behavioral measure
    
    Returns:
        num_epochs, k_hidden
    """
    _, heat_map = get_grid_acc(args, bhv, score, criteria) 
    # second element in tuple is inner cv accuracy
    
    if args.model_type in ['gru','ff','tcn']:
        i_star, j_star = np.unravel_index(
            heat_map.argmax(), heat_map.shape)
        
        return (args.num_epochs[i_star],
                args.k_param[j_star])
    elif args.model_type in ['cpm', 'bbs']:
        i_star = heat_map.argmax()
        
        return args.k_param[i_star]